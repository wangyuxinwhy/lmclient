# LMClient ä»ä½•è€Œæ¥

å›½å†…çš„å¤§æ¨¡å‹å‚å®¶ç™¾èŠ±é½æ”¾ï¼Œå›½å¤–ä¹Ÿæ˜¯å±‚å‡ºä¸ç©·ã€‚ä½†ä¸åŒå¹³å°ä¹‹é—´çš„ä¿¡æ¯æ ¼å¼ï¼Œè°ƒç”¨æ–¹å¼ï¼Œæ¨ç†å‚æ•°ä¹Ÿæ˜¯äº”èŠ±å…«é—¨ï¼Œç¼ºå°‘ç»Ÿä¸€ã€‚å¦‚æœä½ æƒ³è¦åœ¨ä¸šåŠ¡æˆ–è€…åº”ç”¨ä¸­æ”¯æŒä¸åŒçš„å¹³å°ï¼Œä½ éœ€è¦å®‰è£…å„ç§ SDKï¼Œå†å¼€å‘ä¸åŒçš„é€‚é…ä»£ç ï¼è¿™æ ·çš„å·¥ä½œï¼Œç¹çï¼Œæ˜“é”™ï¼Œä¹Ÿå¾ˆæ— èŠã€‚

é‚£ä¹ˆï¼Œå¼€æºç¤¾åŒºä¸­æœ‰æ²¡æœ‰ç°æˆçš„å·¥å…·æ¥è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿç»è¿‡ä¸€ç•ªè°ƒç ”ä¹‹åï¼Œåªæ‰¾åˆ°äº†ä¸€ä¸ªæ²¡æœ‰æ´»è·ƒå¼€å‘çš„é¡¹ç›® [LLM-Fusion_API](https://github.com/ninehills/LLM-Fusion-API) ï¼Œä»¥åŠä¸€ä¸ªåªè¦†ç›–äº†å›½å¤–ä¸»æµå¤§æ¨¡å‹å¹³å°çš„é¡¹ç›® [litellm](https://github.com/BerriAI/litellm)ã€‚

`litellm` çœ‹èµ·æ¥ç›¸å½“ä¸é”™ï¼Œé‚£ç›´æ¥ç»™ litellm æå‡ ä¸ª PR æ¥è¦†ç›–å›½å†…çš„å¤§æ¨¡å‹å¹³å°å§ï¼ä½†æ˜¯å½“æˆ‘çœ‹åˆ° [litellm.completion](https://github.com/BerriAI/litellm/blob/v0.11.1/litellm/main.py#L158) è¿™ä¸ª function åŒ…å« 1000+ è¡Œä»£ç åï¼Œæˆ‘æ”¾å¼ƒäº†è¿™ä¸ªå¤©çœŸçš„æƒ³æ³• ğŸ˜„ã€‚

ç°åœ¨åªå‰©ä¸‹æœ€åä¸€æ¡è·¯äº†ï¼Œæ—¢ç„¶ç›®å‰å¼€æºç¤¾åŒºä¸­æ²¡æœ‰æ»¡è¶³æˆ‘éœ€æ±‚çš„é¡¹ç›®ï¼Œé‚£æˆ‘è‡ªå·±å†™ä¸€ä¸ªå§ï¼

LMClient ç”±æ­¤è¯ç”Ÿ ğŸ‰

## LMClient è®¾è®¡

åœ¨å†³å®šè‡ªå·±å†™ LMClient åï¼Œé¢å¯¹çš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯è¦ææ¸…æ¥šä¸€ä¸ªå¥½ç”¨çš„ **å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯** åº”è¯¥å…·å¤‡å“ªäº›æ€§è´¨ï¼Ÿ

æˆ‘ç®€å•çš„ç½—åˆ—ä¸€ä¸‹ï¼š

1. **ç®€å•**ï¼šæ— éœ€å…³å¿ƒç»†èŠ‚ï¼Œåªéœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£
2. **æ˜“ç”¨**ï¼šæä¾›å¼€ç®±å³ç”¨çš„åŠŸèƒ½ï¼Œå¦‚ *ChatEngine*, *CompletionEngine*, *function call* ç­‰
3. **é«˜æ•ˆ**ï¼šæ”¯æŒæµå¼/éæµå¼æ¨ç†ï¼Œä»¥åŠå¼‚æ­¥/åŒæ­¥æ¨ç†
4. **çµæ´»**ï¼šæ”¯æŒæ¨¡å‹å‚æ•°ï¼Œ*functions*ï¼Œ*system prompt* ç­‰è®¾ç½®
5. **å¯é **ï¼šè¾“å…¥æ£€æŸ¥ï¼Œå‚æ•°æ£€æŸ¥ï¼Œå¼‚å¸¸å¤„ç†
6. **ä¸°å¯Œ**ï¼šå°½å¯èƒ½å¤šçš„æ”¯æŒå„ç§å¹³å°

LMClient çš„è®¾è®¡ä¹Ÿç”±æ­¤å±•å¼€~

### ç®€å•

LMClient æä¾›çš„æ¨¡å‹éƒ½ä½¿ç”¨ç»Ÿä¸€çš„ `chat_completion` è¿›è¡Œè°ƒç”¨ï¼ŒLMClient åœ¨å†…éƒ¨ç»Ÿä¸€äº†ä¸åŒå¹³å°ä¹‹é—´çš„æ¶ˆæ¯æ ¼å¼ï¼Œæ¨ç†å‚æ•°ï¼Œæ¥å£å°è£…ï¼Œè¿”å›è§£æç­‰ï¼

ç°åœ¨ï¼Œä½ å¯ä»¥å†™å‡ºåƒä¸‹æ–¹ä¸€æ ·çš„ä»£ç ï¼Œæ˜¯ä¸æ˜¯å¾ˆç®€å•ï¼Ÿ

```python
from lmclient import MinimaxChat, MinimaxProChat, OpenAIChat, WenxinChat, ZhiPuChat, HunyuanChat, BaichuanChat, ZhiPuCharacterChat

chat_models = [
    MinimaxChat(),
    MinimaxProChat(),
    OpenAIChat(),
    WenxinChat(),
    ZhiPuChat(),
    ZhiPuCharacterChat(),
    HunyuanChat(),
    BaichuanChat(),
]

for model in chat_models:
    model_response = model.chat_completion('ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', temperature=0)
    print(f'{model.model_id}: {model_response.reply}')
```

è¾“å‡º

```
minimax/abab5.5-chat: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
minimax_pro/abab5.5-chat: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
openai/gpt-3.5-turbo: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
wenxin/ERNIE-Bot: ä½ å¥½ï¼Œå¾ˆé«˜å…´å’Œä½ äº¤æµã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
zhipu/chatglm_turbo: " ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"
zhipu-character/characterglm: "ï¼ˆæœ‰ç‚¹å°´å°¬ï¼‰ä½ å¥½ï¼Œä¸å¥½æ„æ€ï¼Œæˆ‘å¥½åƒèµ°é”™åŒ–å¦†é—´äº†\n"
hunyuan/v1: ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ
zhipu/Baichuan2-53B: ä½ å¥½ï¼å¾ˆé«˜å…´å’Œä½ äº¤æµã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£å†³å—ï¼Ÿ
```

### æ˜“ç”¨

LMClient æä¾›äº†å¼€ç®±å³ç”¨çš„åŠŸèƒ½ï¼Œå¦‚ ChatEngine, CompletionEngine, function ç­‰

- **ChatEngine**: èŠå¤©å¼•æ“ï¼Œ`chat_completion` æ˜¯æ— çŠ¶æ€çš„ï¼Œå³æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¼šè®°ä½ä¸Šä¸€æ¬¡çš„å¯¹è¯å†…å®¹ã€‚è€Œ **ChatEngine** åˆ™æ·»åŠ äº†ç®¡ç†å¯¹è¯å†å²ï¼Œå¤„ç† *function call* ï¼Œæµå¼æ‰“å°ç­‰åŠŸèƒ½
- **CompletionEngine**: è¡¥å…¨å¼•æ“ï¼Œåœ¨ `chat_completion` çš„åŠŸèƒ½ä¹‹ä¸Šï¼Œæ·»åŠ äº†å¼‚æ­¥å¹¶å‘æ§åˆ¶ï¼Œæ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ§åˆ¶ï¼Œè¿›åº¦æ¡ç­‰åŠŸèƒ½
- **function**: ä¸€ä¸ªè£…é¥°å™¨ï¼Œå¯ä»¥å°†ä»»æ„ Python å‡½æ•°è½¬æ¢ä¸ºçš„ jsonschema ï¼Œæ–¹ä¾¿ *function call* çš„ä½¿ç”¨
- ...

### é«˜æ•ˆ

LMClient æ”¯æŒæµå¼/éæµå¼æ¨ç†ï¼Œä»¥åŠå¼‚æ­¥/åŒæ­¥æ¨ç†ï¼Œæ³¨æ„è¿™é‡Œçš„ api åç§°æœ‰æ‰€å˜åŒ–ï¼Œè€Œå‘½åæ˜¯éå¸¸è§„å¾‹çš„ã€‚*async* å‰ç¼€ä»£è¡¨å¼‚æ­¥ï¼Œ*stream* å‰ç¼€ä»£è¡¨æµå¼ã€‚



- éæµå¼åŒæ­¥æ¨ç†
    ```python {4}
    from lmclient import MinimaxChat

    model = MinimaxChat()
    response = model.chat_completion('ä½ å¥½')
    print(response.reply)
    ```

- æµå¼åŒæ­¥æ¨ç† 
    ```python {4}
    from lmclient import MinimaxChat

    model = MinimaxChat()
    stream_responses = model.stream_chat_completion('ä½ å¥½')
    for stream_response in stream_responses:
        print(stream_response.stream.delta, end='', flush=True)
        if stream_response.is_finish:
            response = stream_response
    ```

- éæµå¼å¼‚æ­¥æ¨ç†
    ```python {6}
    import asyncio

    from lmclient import MinimaxChat

    model = MinimaxChat()
    co_response = model.async_chat_completion('ä½ å¥½')
    response = asyncio.run(co_response)
    print(response)
    ```

- æµå¼å¼‚æ­¥æ¨ç†
    ```python {7}
    import asyncio

    from lmclient import MinimaxChat

    model = MinimaxChat()
    async def async_chat():
        async for stream_response in model.async_stream_chat_completion('ä½ å¥½'):
            if stream_response.is_finish:
                response = stream_response
                return response
            else:
                print(stream_response.stream.delta, end='', flush=True)
    response = asyncio.run(async_chat())
    print(response)
    ```


### çµæ´»

LMClient æ”¯æŒæ¨ç†å‚æ•°çš„è®¾ç½®ï¼Œè¿™æ–¹é¢çš„ç»†èŠ‚å¾ˆå¤šã€‚ä¾‹å¦‚ **max_tokens** åœ¨ä¸åŒçš„å¹³å°æœ‰ä¸åŒçš„åç§°ï¼Œ**temperature** åœ¨æœ‰äº›å¹³å°ä¸èƒ½è®¾ç½®ä¸º 0 ç­‰ç­‰ã€‚ä¸è¿‡ï¼Œä½œä¸ºç”¨æˆ·ä½ æ— éœ€å…³å¿ƒè¿™äº›ç»†èŠ‚ï¼ŒLMClient ä¼šä¸ºä½ å¤„ç†æ‰€æœ‰éº»çƒ¦ã€‚

```python {5}
from lmclient import MinimaxChat, OpenAIChat

models = [MinimaxChat(), OpenAIChat()]
for model in models:
    model_response = model.chat_completion('ä½ å¥½', temperature=0, max_tokens=10)
    print(f'{model.model_id}: {model_response.reply}')
```

è¾“å‡º

```
minimax/abab5.5-chat: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ
openai/gpt-3.5-turbo: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥
```

### å¯é 

LMClient å†…éƒ¨å¯¹è¾“å…¥ï¼Œæ¨¡å‹å‚æ•°ï¼Œå¼‚å¸¸ç­‰è¿›è¡Œäº†æ£€æŸ¥å’Œå¤„ç†ï¼Œä¿è¯äº†ä»£ç çš„å¯é æ€§ã€‚

å½“ä½ é”™è¯¯çš„å°† **temperature** è®¾ç½®ä¸ºè´Ÿæ•°æ—¶ï¼Œä½ å°†ä¼šå¾—åˆ°å¦‚ä¸‹çš„é”™è¯¯æç¤ºã€‚

```python
from lmclient import OpenAIChat

model = OpenAIChat()
model.chat_completion('ä½ å¥½', temperature=-1)
```

è¾“å‡º

```
ValidationError: 1 validation error for OpenAIChatParameters
temperature
  Input should be greater than or equal to 0 [type=greater_than_equal, input_value=-1, input_type=int]
    For further information visit https://errors.pydantic.dev/2.4/v/greater_than_equal
```

### ä¸°å¯Œ

LMClient å½“å‰æ”¯æŒä»¥ä¸‹ **7** ä¸ªå¹³å°çš„ **9** ç§æ¨¡å‹ï¼Œæ³¨æ„æœ‰åŒä¸€å¹³å°ä¼šæä¾›ä¸åŒç§ç±»çš„æ¨¡å‹ï¼Œæ¯”å¦‚ `ZhiPuCharacterChat` å°±ä¸“é—¨ä¸ºè§’è‰²æ‰®æ¼”è€Œç”Ÿã€‚

å½“ç„¶åŒç§ç±»å‹æ¨¡å‹è¿˜æœ‰ä¸åŒçš„ç‰ˆæœ¬ï¼Œæ¯”å¦‚ç™¾åº¦æœ‰ *Ernie-Bot-4* å’Œ *Ernie-Bot-3.5* ç­‰ã€‚

å®Œæˆçš„åˆ—è¡¨å¦‚ä¸‹ï¼š

- MinimaxChat
- MinimaxProChat
- AzureChat
- OpenAIChat
- ZhiPuChat
- ZhiPuCharacterChat
- WenxinChat
- HunyuanChat
- BaichuanChat

LMClient è¿˜åœ¨ä¸°å¯Œä¸­ï¼Œæ¬¢è¿å¤§å®¶æ PR æˆ–è€… Issue æ¥æ”¯æŒæ›´å¤šçš„å¹³å°ï¼


## LMClient ä»£ç 

### å¯è¯»æ€§

LMClient çš„ä»£ç ä½¿ç”¨ [ruff](https://docs.astral.sh/ruff/) è¿›è¡Œè¯¦å°½çš„ä»£ç é£æ ¼æ£€æŸ¥å’Œæ ¼å¼åŒ–ï¼Œä½¿ç”¨ [pyright](https://github.com/microsoft/pyright) åšé™æ€ç±»å‹æ£€æŸ¥ï¼Œå¹¶åšåˆ°äº† 100% çš„ç±»å‹æ³¨è§£ (type hints)ã€‚å› æ­¤ï¼Œå½“æ‚¨ä½¿ç”¨ LMClient è¿›è¡Œå¼€å‘æ—¶ï¼Œä½ å°†è·å¾—å®Œç¾çš„å¼€å‘ä½“éªŒã€‚

æ€»ä¹‹ï¼ŒLMClient çš„ä»£ç åœ¨å¯è¯»æ€§æ–¹é¢åšäº†æœ€å¤§çš„åŠªåŠ›ï¼Œä»£ç çš„è®¾è®¡ä¹Ÿæ˜¯ç®€æ´æ˜“æ‡‚çš„ï¼Œå“ªæ€•æ·±å…¥æºç ï¼Œä¹Ÿä¸ä¼šè®©ä½ æ„Ÿåˆ°å›°æƒ‘ã€‚

### æµ‹è¯•

LMClient çš„ä»£ç ä½¿ç”¨ `pytest` è¿›è¡Œå•å…ƒæµ‹è¯•ï¼Œè¦†ç›–äº† LMClient æ‰€æœ‰å…¬å¼€æä¾›çš„æ¥å£ã€‚

